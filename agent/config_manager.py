#!/usr/bin/env python3
"""
Configuration management for MCP Agent
設定管理モジュール
"""

import os
import yaml
from dataclasses import dataclass, field
from typing import Dict, Any


@dataclass
class DisplayConfig:
    ui_mode: str = "basic"
    show_timing: bool = True
    show_thinking: bool = True


@dataclass
class RetryStrategyConfig:
    max_retries: int = 3
    progressive_temperature: bool = True
    initial_temperature: float = 0.1
    temperature_increment: float = 0.2


@dataclass
class ExecutionConfig:
    max_retries: int = 3
    timeout_seconds: int = 30
    fallback_enabled: bool = False
    max_tasks: int = 10
    retry_strategy: RetryStrategyConfig = field(default_factory=RetryStrategyConfig)


@dataclass
class LLMConfig:
    model: str = "gpt-4o-mini"
    temperature: float = 0.2
    force_json: bool = True
    reasoning_effort: str = "minimal"
    max_completion_tokens: int = 5000


@dataclass
class InterruptHandlingConfig:
    timeout: float = 10.0
    non_interactive_default: str = "abort"


@dataclass
class ConversationConfig:
    context_limit: int = 10
    max_history: int = 50


@dataclass
class ErrorHandlingConfig:
    auto_correct_params: bool = True
    retry_interval: float = 1.0


@dataclass
class DevelopmentConfig:
    verbose: bool = True
    log_level: str = "INFO"
    show_api_calls: bool = True


@dataclass
class ResultDisplayConfig:
    max_result_length: int = 1000
    show_truncated_info: bool = True


@dataclass
class Config:
    display: DisplayConfig = field(default_factory=DisplayConfig)
    execution: ExecutionConfig = field(default_factory=ExecutionConfig)
    llm: LLMConfig = field(default_factory=LLMConfig)
    conversation: ConversationConfig = field(default_factory=ConversationConfig)
    error_handling: ErrorHandlingConfig = field(default_factory=ErrorHandlingConfig)
    development: DevelopmentConfig = field(default_factory=DevelopmentConfig)
    result_display: ResultDisplayConfig = field(default_factory=ResultDisplayConfig)
    interrupt_handling: InterruptHandlingConfig = field(default_factory=InterruptHandlingConfig)


class ConfigManager:
    """設定管理クラス"""

    @staticmethod
    def load(config_path: str) -> Config:
        if not os.path.exists(config_path):
            # デフォルト設定を返す
            return Config()

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                yaml_data = yaml.safe_load(f)

            return ConfigManager._create_config_from_dict(yaml_data or {})

        except Exception as e:
            raise ValueError(f"設定ファイル読み込みエラー: {e}")

    @staticmethod
    def _create_config_from_dict(data: Dict[str, Any]) -> Config:
        config = Config()

        if "display" in data:
            display_data = data["display"]
            config.display = DisplayConfig(
                ui_mode=display_data.get("ui_mode", "basic"),
                show_timing=display_data.get("show_timing", True),
                show_thinking=display_data.get("show_thinking", True)
            )

        if "execution" in data:
            exec_data = data["execution"]
            retry_data = exec_data.get("retry_strategy", {})

            config.execution = ExecutionConfig(
                max_retries=exec_data.get("max_retries", 3),
                timeout_seconds=exec_data.get("timeout_seconds", 30),
                fallback_enabled=exec_data.get("fallback_enabled", False),
                max_tasks=exec_data.get("max_tasks", 10),
                retry_strategy=RetryStrategyConfig(
                    max_retries=retry_data.get("max_retries", 3),
                    progressive_temperature=retry_data.get("progressive_temperature", True),
                    initial_temperature=retry_data.get("initial_temperature", 0.1),
                    temperature_increment=retry_data.get("temperature_increment", 0.2)
                )
            )

        if "llm" in data:
            llm_data = data["llm"]
            config.llm = LLMConfig(
                model=llm_data.get("model", "gpt-4o-mini"),
                temperature=llm_data.get("temperature", 0.2),
                force_json=llm_data.get("force_json", True),
                reasoning_effort=llm_data.get("reasoning_effort", "minimal"),
                max_completion_tokens=llm_data.get("max_completion_tokens", 5000)
            )

        if "conversation" in data:
            conv_data = data["conversation"]
            config.conversation = ConversationConfig(
                context_limit=conv_data.get("context_limit", 10),
                max_history=conv_data.get("max_history", 50)
            )

        if "error_handling" in data:
            error_data = data["error_handling"]
            config.error_handling = ErrorHandlingConfig(
                auto_correct_params=error_data.get("auto_correct_params", True),
                retry_interval=error_data.get("retry_interval", 1.0)
            )

        if "development" in data:
            dev_data = data["development"]
            config.development = DevelopmentConfig(
                verbose=dev_data.get("verbose", True),
                log_level=dev_data.get("log_level", "INFO"),
                show_api_calls=dev_data.get("show_api_calls", True)
            )

        if "result_display" in data:
            result_data = data["result_display"]
            config.result_display = ResultDisplayConfig(
                max_result_length=result_data.get("max_result_length", 1000),
                show_truncated_info=result_data.get("show_truncated_info", True)
            )

        if "interrupt_handling" in data:
            interrupt_data = data["interrupt_handling"]
            config.interrupt_handling = InterruptHandlingConfig(
                timeout=interrupt_data.get("timeout", 10.0),
                non_interactive_default=interrupt_data.get("non_interactive_default", "abort")
            )

        return config
